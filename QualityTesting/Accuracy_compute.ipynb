{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11879f16-e4e1-4cc0-b208-4ae991dc1500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:42:27.350102Z",
     "iopub.status.busy": "2025-04-21T10:42:27.349402Z",
     "iopub.status.idle": "2025-04-21T10:43:18.976449Z",
     "shell.execute_reply": "2025-04-21T10:43:18.975528Z",
     "shell.execute_reply.started": "2025-04-21T10:42:27.350078Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-04-21 10:43:06.095018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745232186.316169    4834 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745232186.401370    4834 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745232186.901423    4834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745232186.901446    4834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745232186.901448    4834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745232186.901449    4834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-21 10:43:06.958644: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama import LlamaForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaMLP\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from model import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e28149-ad6d-4bd2-8ae0-95466d2b2689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:43:18.978488Z",
     "iopub.status.busy": "2025-04-21T10:43:18.977898Z",
     "iopub.status.idle": "2025-04-21T10:44:42.225795Z",
     "shell.execute_reply": "2025-04-21T10:44:42.224995Z",
     "shell.execute_reply.started": "2025-04-21T10:43:18.978466Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\", torch_dtype=dtype)\n",
    "base_model.to(device)\n",
    "base_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2539f17e-6dd4-45d9-8f41-b99c22534983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:44:42.226925Z",
     "iopub.status.busy": "2025-04-21T10:44:42.226613Z",
     "iopub.status.idle": "2025-04-21T10:45:07.373975Z",
     "shell.execute_reply": "2025-04-21T10:45:07.373329Z",
     "shell.execute_reply.started": "2025-04-21T10:44:42.226905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoHeadModel(\n",
       "  (base_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  )\n",
       "  (main_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  (speculator_head): PredictorHead(\n",
       "    (decoder): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (proj): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speculator_head = PredictorHead(base_model.model.config)\n",
    "speculator_head.load_state_dict(torch.load(\"../new_nn_decoder_head1.pth\"))\n",
    "speculator_head.to(device, dtype=dtype)\n",
    "specModel = TwoHeadModel(base_model, speculator_head)\n",
    "specModel.to(device, dtype=dtype)\n",
    "specModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9dda9a-b7c8-41b3-a554-87579f5c0cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:07.375750Z",
     "iopub.status.busy": "2025-04-21T10:45:07.375436Z",
     "iopub.status.idle": "2025-04-21T10:45:07.428646Z",
     "shell.execute_reply": "2025-04-21T10:45:07.428045Z",
     "shell.execute_reply.started": "2025-04-21T10:45:07.375730Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_prefix_embeddings = torch.load(\"prefix_embeddings_main.pt\")  # this is a torch.Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1758307b-c994-4850-9b79-618ab0982257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:07.429905Z",
     "iopub.status.busy": "2025-04-21T10:45:07.429279Z",
     "iopub.status.idle": "2025-04-21T10:45:07.497685Z",
     "shell.execute_reply": "2025-04-21T10:45:07.496949Z",
     "shell.execute_reply.started": "2025-04-21T10:45:07.429886Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrefixTuningModel(nn.Module):\n",
    "    def __init__(self, base_model, speculator_head, prefix_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_model: The pre-trained LlamaForCausalLM model.\n",
    "            speculator_head: The additional head (predicting the second token).\n",
    "            prefix_length: Number of trainable prefix tokens.\n",
    "        \"\"\"\n",
    "        super(PrefixTuningModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.prefix_length = prefix_length\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        self.prefix_embeddings = nn.Parameter(torch.randn(prefix_length, self.hidden_size, dtype=dtype))\n",
    "        self.main_head = base_model.lm_head\n",
    "        self.speculator_head = speculator_head\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        input_embeds = self.base_model.model.embed_tokens(input_ids)  # shape: [batch, seq_len, hidden_size]\n",
    "        prefix_embeds = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        concat_embeds = torch.cat([prefix_embeds, input_embeds], dim=1)\n",
    "        if attention_mask is not None:\n",
    "            prefix_mask = torch.ones(batch_size, self.prefix_length, device=attention_mask.device)\n",
    "            attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "        outputs = self.base_model.model(inputs_embeds=concat_embeds, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # shape: [batch, (prefix+seq_len), hidden_size]\n",
    "        logits_main = self.main_head(hidden_states)\n",
    "    \n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        logits_speculator = self.speculator_head(hidden_states)\n",
    "        return logits_main, logits_speculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1b94f7-d87c-49e1-a571-72ad1c715ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:07.498648Z",
     "iopub.status.busy": "2025-04-21T10:45:07.498332Z",
     "iopub.status.idle": "2025-04-21T10:45:07.544272Z",
     "shell.execute_reply": "2025-04-21T10:45:07.543615Z",
     "shell.execute_reply.started": "2025-04-21T10:45:07.498629Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrefixTuningModel(\n",
       "  (base_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  )\n",
       "  (main_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  (speculator_head): PredictorHead(\n",
       "    (decoder): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (proj): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "prefix_length = 8\n",
    "\n",
    "p_tuning_model = PrefixTuningModel(base_model, speculator_head, prefix_length)\n",
    "\n",
    "p_tuning_model.prefix_embeddings.data.copy_(loaded_prefix_embeddings.to(p_tuning_model.prefix_embeddings.device))\n",
    "\n",
    "\n",
    "p_tuning_model.to(device)\n",
    "\n",
    "for param in p_tuning_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in p_tuning_model.speculator_head.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for param in p_tuning_model.parametrs():\n",
    "#     paream.requires_grad = False\n",
    "    \n",
    "p_tuning_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1401a7aa-6afc-4628-976f-091dcc412762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:07.545569Z",
     "iopub.status.busy": "2025-04-21T10:45:07.544932Z",
     "iopub.status.idle": "2025-04-21T10:45:11.982081Z",
     "shell.execute_reply": "2025-04-21T10:45:11.980894Z",
     "shell.execute_reply.started": "2025-04-21T10:45:07.545550Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee090bb0-f50b-42bc-8ca7-b660a86bd44f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:11.983322Z",
     "iopub.status.busy": "2025-04-21T10:45:11.982991Z",
     "iopub.status.idle": "2025-04-21T10:45:12.014656Z",
     "shell.execute_reply": "2025-04-21T10:45:12.013929Z",
     "shell.execute_reply.started": "2025-04-21T10:45:11.983302Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f40849-47f0-4dd8-ae65-aab17f931d0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:12.016228Z",
     "iopub.status.busy": "2025-04-21T10:45:12.015456Z",
     "iopub.status.idle": "2025-04-21T10:45:12.048042Z",
     "shell.execute_reply": "2025-04-21T10:45:12.047255Z",
     "shell.execute_reply.started": "2025-04-21T10:45:12.016207Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.autocast(device_type=\"cuda\")\n",
    "def infer_prefix_tuning_model(model, tokenizer, input_string, max_length=300, eos_token_id=2):\n",
    "    \"\"\"\n",
    "    Inference function for PrefixTuningModel that generates a sequence token-by-token until EOS token.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PrefixTuningModel.\n",
    "        tokenizer: The tokenizer for LLaMA model.\n",
    "        input_string: The input string (prompt) to generate the response for.\n",
    "        max_length: Maximum length of the generated sequence (including prefix).\n",
    "        eos_token_id: The ID of the EOS token, used to stop generation.\n",
    "    \n",
    "    Returns:\n",
    "        output_string: The model's generated output as a string.\n",
    "    \"\"\"\n",
    "    # Tokenize the input string\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # Create an attention mask (same size as input_ids)\n",
    "    attention_mask = inputs.get(\"attention_mask\", torch.ones_like(input_ids)).to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the input sequence with the input_ids\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    # Loop for generating one token at a time\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # Get the logits from the model (we only want the last token logits)\n",
    "            logits_main, _ = model(generated_ids, attention_mask)\n",
    "\n",
    "            # Get the logits for the last token (the current token we want to predict)\n",
    "            logits = logits_main[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
    "\n",
    "            # Get the predicted next token using argmax (greedy decoding)\n",
    "            next_token_id = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "            # Append the predicted token to the sequence\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "\n",
    "            # If the predicted token is the EOS token, stop the generation\n",
    "            if next_token_id.item() == eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode the generated token IDs into a string\n",
    "    output_string = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653e8751-844c-4531-8b58-0e9e4942b08f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:12.050370Z",
     "iopub.status.busy": "2025-04-21T10:45:12.050036Z",
     "iopub.status.idle": "2025-04-21T10:45:12.069720Z",
     "shell.execute_reply": "2025-04-21T10:45:12.069021Z",
     "shell.execute_reply.started": "2025-04-21T10:45:12.050351Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_final_answer(text):\n",
    "    match = re.search(r\"####\\s*(\\d+)\", text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None  # or raise an error / return a default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1db5cee-3aae-4a75-aa90-d4a81141f11d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:12.070708Z",
     "iopub.status.busy": "2025-04-21T10:45:12.070402Z",
     "iopub.status.idle": "2025-04-21T10:45:12.102204Z",
     "shell.execute_reply": "2025-04-21T10:45:12.101533Z",
     "shell.execute_reply.started": "2025-04-21T10:45:12.070690Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_such_number_in_string(number, string) -> bool:\n",
    "    pattern = r'\\b{}\\b'.format(re.escape(str(number)))\n",
    "    return re.search(pattern, string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b9fdd1d-04cc-41af-9472-7ee82d80d7a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:12.103200Z",
     "iopub.status.busy": "2025-04-21T10:45:12.102887Z",
     "iopub.status.idle": "2025-04-21T10:45:12.139994Z",
     "shell.execute_reply": "2025-04-21T10:45:12.139319Z",
     "shell.execute_reply.started": "2025-04-21T10:45:12.103181Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1319\n"
     ]
    }
   ],
   "source": [
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb3b06a-9c25-43af-a0dc-a8af5be1260f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:12.140954Z",
     "iopub.status.busy": "2025-04-21T10:45:12.140653Z",
     "iopub.status.idle": "2025-04-21T10:45:12.183945Z",
     "shell.execute_reply": "2025-04-21T10:45:12.183249Z",
     "shell.execute_reply.started": "2025-04-21T10:45:12.140935Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_compute():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(len(val_dataset)):\n",
    "        item = val_dataset[i]\n",
    "        correct_ans = extract_final_answer(item['answer'])\n",
    "        model_answer = infer_prefix_tuning_model(p_tuning_model, tokenizer, item['question'], max_length=300, eos_token_id=2)\n",
    "        if (is_such_number_in_string(correct_ans, model_answer)):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        print(f'correct: {correct} total: {total}')\n",
    "\n",
    "        \n",
    "        if (i > 300):\n",
    "            break\n",
    "\n",
    "    \n",
    "    return (correct / total)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb27e11d-8b65-4cfd-a5a0-dc7e084740c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:45:12.184891Z",
     "iopub.status.busy": "2025-04-21T10:45:12.184588Z",
     "iopub.status.idle": "2025-04-21T11:17:30.508577Z",
     "shell.execute_reply": "2025-04-21T11:17:30.507263Z",
     "shell.execute_reply.started": "2025-04-21T10:45:12.184873Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 0 total: 1\n",
      "correct: 0 total: 2\n",
      "correct: 0 total: 3\n",
      "correct: 0 total: 4\n",
      "correct: 1 total: 5\n",
      "correct: 1 total: 6\n",
      "correct: 1 total: 7\n",
      "correct: 1 total: 8\n",
      "correct: 1 total: 9\n",
      "correct: 1 total: 10\n",
      "correct: 1 total: 11\n",
      "correct: 1 total: 12\n",
      "correct: 1 total: 13\n",
      "correct: 1 total: 14\n",
      "correct: 1 total: 15\n",
      "correct: 1 total: 16\n",
      "correct: 1 total: 17\n",
      "correct: 1 total: 18\n",
      "correct: 1 total: 19\n",
      "correct: 2 total: 20\n",
      "correct: 3 total: 21\n",
      "correct: 3 total: 22\n",
      "correct: 3 total: 23\n",
      "correct: 4 total: 24\n",
      "correct: 4 total: 25\n",
      "correct: 5 total: 26\n",
      "correct: 5 total: 27\n",
      "correct: 5 total: 28\n",
      "correct: 5 total: 29\n",
      "correct: 5 total: 30\n",
      "correct: 5 total: 31\n",
      "correct: 6 total: 32\n",
      "correct: 7 total: 33\n",
      "correct: 7 total: 34\n",
      "correct: 7 total: 35\n",
      "correct: 8 total: 36\n",
      "correct: 8 total: 37\n",
      "correct: 9 total: 38\n",
      "correct: 9 total: 39\n",
      "correct: 9 total: 40\n",
      "correct: 9 total: 41\n",
      "correct: 9 total: 42\n",
      "correct: 9 total: 43\n",
      "correct: 9 total: 44\n",
      "correct: 10 total: 45\n",
      "correct: 10 total: 46\n",
      "correct: 10 total: 47\n",
      "correct: 10 total: 48\n",
      "correct: 10 total: 49\n",
      "correct: 11 total: 50\n",
      "correct: 11 total: 51\n",
      "correct: 11 total: 52\n",
      "correct: 12 total: 53\n",
      "correct: 13 total: 54\n",
      "correct: 13 total: 55\n",
      "correct: 13 total: 56\n",
      "correct: 14 total: 57\n",
      "correct: 14 total: 58\n",
      "correct: 14 total: 59\n",
      "correct: 14 total: 60\n",
      "correct: 14 total: 61\n",
      "correct: 14 total: 62\n",
      "correct: 14 total: 63\n",
      "correct: 14 total: 64\n",
      "correct: 14 total: 65\n",
      "correct: 14 total: 66\n",
      "correct: 14 total: 67\n",
      "correct: 14 total: 68\n",
      "correct: 14 total: 69\n",
      "correct: 14 total: 70\n",
      "correct: 14 total: 71\n",
      "correct: 14 total: 72\n",
      "correct: 14 total: 73\n",
      "correct: 14 total: 74\n",
      "correct: 14 total: 75\n",
      "correct: 14 total: 76\n",
      "correct: 14 total: 77\n",
      "correct: 14 total: 78\n",
      "correct: 14 total: 79\n",
      "correct: 15 total: 80\n",
      "correct: 15 total: 81\n",
      "correct: 15 total: 82\n",
      "correct: 15 total: 83\n",
      "correct: 15 total: 84\n",
      "correct: 15 total: 85\n",
      "correct: 15 total: 86\n",
      "correct: 15 total: 87\n",
      "correct: 15 total: 88\n",
      "correct: 15 total: 89\n",
      "correct: 15 total: 90\n",
      "correct: 15 total: 91\n",
      "correct: 15 total: 92\n",
      "correct: 16 total: 93\n",
      "correct: 16 total: 94\n",
      "correct: 16 total: 95\n",
      "correct: 16 total: 96\n",
      "correct: 17 total: 97\n",
      "correct: 17 total: 98\n",
      "correct: 18 total: 99\n",
      "correct: 18 total: 100\n",
      "correct: 18 total: 101\n",
      "correct: 18 total: 102\n",
      "correct: 18 total: 103\n",
      "correct: 18 total: 104\n",
      "correct: 18 total: 105\n",
      "correct: 18 total: 106\n",
      "correct: 18 total: 107\n",
      "correct: 19 total: 108\n",
      "correct: 19 total: 109\n",
      "correct: 19 total: 110\n",
      "correct: 19 total: 111\n",
      "correct: 19 total: 112\n",
      "correct: 19 total: 113\n",
      "correct: 19 total: 114\n",
      "correct: 20 total: 115\n",
      "correct: 20 total: 116\n",
      "correct: 20 total: 117\n",
      "correct: 20 total: 118\n",
      "correct: 21 total: 119\n",
      "correct: 21 total: 120\n",
      "correct: 21 total: 121\n",
      "correct: 21 total: 122\n",
      "correct: 21 total: 123\n",
      "correct: 21 total: 124\n",
      "correct: 21 total: 125\n",
      "correct: 21 total: 126\n",
      "correct: 21 total: 127\n",
      "correct: 21 total: 128\n",
      "correct: 21 total: 129\n",
      "correct: 22 total: 130\n",
      "correct: 23 total: 131\n",
      "correct: 23 total: 132\n",
      "correct: 23 total: 133\n",
      "correct: 23 total: 134\n",
      "correct: 23 total: 135\n",
      "correct: 23 total: 136\n",
      "correct: 23 total: 137\n",
      "correct: 23 total: 138\n",
      "correct: 23 total: 139\n",
      "correct: 23 total: 140\n",
      "correct: 24 total: 141\n",
      "correct: 24 total: 142\n",
      "correct: 24 total: 143\n",
      "correct: 24 total: 144\n",
      "correct: 24 total: 145\n",
      "correct: 24 total: 146\n",
      "correct: 25 total: 147\n",
      "correct: 25 total: 148\n",
      "correct: 25 total: 149\n",
      "correct: 25 total: 150\n",
      "correct: 26 total: 151\n",
      "correct: 27 total: 152\n",
      "correct: 28 total: 153\n",
      "correct: 28 total: 154\n",
      "correct: 28 total: 155\n",
      "correct: 28 total: 156\n",
      "correct: 28 total: 157\n",
      "correct: 28 total: 158\n",
      "correct: 28 total: 159\n",
      "correct: 28 total: 160\n",
      "correct: 29 total: 161\n",
      "correct: 29 total: 162\n",
      "correct: 29 total: 163\n",
      "correct: 29 total: 164\n",
      "correct: 29 total: 165\n",
      "correct: 29 total: 166\n",
      "correct: 29 total: 167\n",
      "correct: 29 total: 168\n",
      "correct: 30 total: 169\n",
      "correct: 30 total: 170\n",
      "correct: 30 total: 171\n",
      "correct: 30 total: 172\n",
      "correct: 30 total: 173\n",
      "correct: 30 total: 174\n",
      "correct: 30 total: 175\n",
      "correct: 30 total: 176\n",
      "correct: 30 total: 177\n",
      "correct: 30 total: 178\n",
      "correct: 30 total: 179\n",
      "correct: 30 total: 180\n",
      "correct: 31 total: 181\n",
      "correct: 31 total: 182\n",
      "correct: 31 total: 183\n",
      "correct: 31 total: 184\n",
      "correct: 31 total: 185\n",
      "correct: 32 total: 186\n",
      "correct: 33 total: 187\n",
      "correct: 33 total: 188\n",
      "correct: 33 total: 189\n",
      "correct: 33 total: 190\n",
      "correct: 33 total: 191\n",
      "correct: 34 total: 192\n",
      "correct: 34 total: 193\n",
      "correct: 34 total: 194\n",
      "correct: 34 total: 195\n",
      "correct: 34 total: 196\n",
      "correct: 34 total: 197\n",
      "correct: 34 total: 198\n",
      "correct: 34 total: 199\n",
      "correct: 34 total: 200\n",
      "correct: 34 total: 201\n",
      "correct: 34 total: 202\n",
      "correct: 34 total: 203\n",
      "correct: 34 total: 204\n",
      "correct: 34 total: 205\n",
      "correct: 34 total: 206\n",
      "correct: 34 total: 207\n",
      "correct: 34 total: 208\n",
      "correct: 34 total: 209\n",
      "correct: 34 total: 210\n",
      "correct: 35 total: 211\n",
      "correct: 36 total: 212\n",
      "correct: 37 total: 213\n",
      "correct: 37 total: 214\n",
      "correct: 37 total: 215\n",
      "correct: 37 total: 216\n",
      "correct: 37 total: 217\n",
      "correct: 37 total: 218\n",
      "correct: 38 total: 219\n",
      "correct: 38 total: 220\n",
      "correct: 38 total: 221\n",
      "correct: 39 total: 222\n",
      "correct: 39 total: 223\n",
      "correct: 39 total: 224\n",
      "correct: 39 total: 225\n",
      "correct: 39 total: 226\n",
      "correct: 39 total: 227\n",
      "correct: 39 total: 228\n",
      "correct: 39 total: 229\n",
      "correct: 39 total: 230\n",
      "correct: 39 total: 231\n",
      "correct: 39 total: 232\n",
      "correct: 40 total: 233\n",
      "correct: 40 total: 234\n",
      "correct: 40 total: 235\n",
      "correct: 40 total: 236\n",
      "correct: 40 total: 237\n",
      "correct: 40 total: 238\n",
      "correct: 40 total: 239\n",
      "correct: 40 total: 240\n",
      "correct: 40 total: 241\n",
      "correct: 40 total: 242\n",
      "correct: 41 total: 243\n",
      "correct: 41 total: 244\n",
      "correct: 41 total: 245\n",
      "correct: 41 total: 246\n",
      "correct: 41 total: 247\n",
      "correct: 41 total: 248\n",
      "correct: 42 total: 249\n",
      "correct: 43 total: 250\n",
      "correct: 43 total: 251\n",
      "correct: 43 total: 252\n",
      "correct: 43 total: 253\n",
      "correct: 43 total: 254\n",
      "correct: 43 total: 255\n",
      "correct: 43 total: 256\n",
      "correct: 43 total: 257\n",
      "correct: 43 total: 258\n",
      "correct: 43 total: 259\n",
      "correct: 43 total: 260\n",
      "correct: 43 total: 261\n",
      "correct: 43 total: 262\n",
      "correct: 44 total: 263\n",
      "correct: 44 total: 264\n",
      "correct: 44 total: 265\n",
      "correct: 44 total: 266\n",
      "correct: 44 total: 267\n",
      "correct: 44 total: 268\n",
      "correct: 44 total: 269\n",
      "correct: 44 total: 270\n",
      "correct: 44 total: 271\n",
      "correct: 44 total: 272\n",
      "correct: 44 total: 273\n",
      "correct: 44 total: 274\n",
      "correct: 44 total: 275\n",
      "correct: 44 total: 276\n",
      "correct: 44 total: 277\n",
      "correct: 44 total: 278\n",
      "correct: 45 total: 279\n",
      "correct: 46 total: 280\n",
      "correct: 47 total: 281\n",
      "correct: 47 total: 282\n",
      "correct: 47 total: 283\n",
      "correct: 47 total: 284\n",
      "correct: 47 total: 285\n",
      "correct: 47 total: 286\n",
      "correct: 47 total: 287\n",
      "correct: 48 total: 288\n",
      "correct: 49 total: 289\n",
      "correct: 49 total: 290\n",
      "correct: 49 total: 291\n",
      "correct: 49 total: 292\n",
      "correct: 49 total: 293\n",
      "correct: 49 total: 294\n",
      "correct: 50 total: 295\n",
      "correct: 50 total: 296\n",
      "correct: 50 total: 297\n",
      "correct: 50 total: 298\n",
      "correct: 50 total: 299\n",
      "correct: 50 total: 300\n",
      "correct: 50 total: 301\n",
      "correct: 50 total: 302\n",
      "0.16556291390728478\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80473120-6691-41bc-a1cc-7b4ae94401ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T08:23:20.951449Z",
     "iopub.status.busy": "2025-04-21T08:23:20.951020Z",
     "iopub.status.idle": "2025-04-21T08:23:21.013611Z",
     "shell.execute_reply": "2025-04-21T08:23:21.012643Z",
     "shell.execute_reply.started": "2025-04-21T08:23:20.951427Z"
    },
    "tags": []
   },
   "source": [
    "correct: 27 total: 152\n",
    "0.17763157894736842"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7f084-5445-41ef-9c10-94d6d7d144a1",
   "metadata": {},
   "source": [
    "correct: 50 total: 300\n",
    "\n",
    "correct: 50 total: 301\n",
    "\n",
    "correct: 50 total: 302\n",
    "\n",
    "0.16556291390728478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaca8c93-5d60-489d-bbce-5b855643f540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T11:17:30.509814Z",
     "iopub.status.busy": "2025-04-21T11:17:30.509463Z",
     "iopub.status.idle": "2025-04-21T11:17:31.494403Z",
     "shell.execute_reply": "2025-04-21T11:17:31.492277Z",
     "shell.execute_reply.started": "2025-04-21T11:17:30.509793Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4834/655216181.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'correct' is not defined"
     ]
    }
   ],
   "source": [
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272e401-0e12-4f1a-b970-165900868355",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T11:17:31.494951Z",
     "iopub.status.idle": "2025-04-21T11:17:31.495289Z",
     "shell.execute_reply": "2025-04-21T11:17:31.495123Z",
     "shell.execute_reply.started": "2025-04-21T11:17:31.495109Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(infer_prefix_tuning_model(p_tuning_model, tokenizer, \"What is the result of 2 + 2? Put an answer as last token of your output - after ###\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764ccef-2c3c-42ba-9986-8ff1ecaa4870",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T11:17:31.496210Z",
     "iopub.status.idle": "2025-04-21T11:17:31.496538Z",
     "shell.execute_reply": "2025-04-21T11:17:31.496374Z",
     "shell.execute_reply.started": "2025-04-21T11:17:31.496360Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(infer_prefix_tuning_model(p_tuning_model, tokenizer, \"What is the capital of France? Put an answer as last token of your output - after ###\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53b3fe-5b0a-4dcc-a179-5fd6de1e2b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
