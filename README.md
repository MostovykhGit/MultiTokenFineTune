# MultiTokenFineTune

***Суть проекта***: дообучаем модели с помощью Multi Token Prediction.

***Проверяемая гипотеза***: Использование Multi Token Prediction улучшит/ускорит дообучение предобученной модели под новую задачу. 

Как именно это будет работать:
1) Поступает предобученная модель
2) Добавляем к ней несколько спекулятивных голов, обученных для архитектуры модели заранее нами
3) Обучаем модель используя лоссы от главной и спекулятивных голов. 

На выходе хотим получить более качественную основную модель.

## Структура проекта

* model.py - добавляем к модели спекулятивную голову
* train_spec_head.ipynb - ноутбук с обучением спекулятивной головы на Aeala/ShareGPT_Vicuna_unfiltered
* new_run.ipynb - ноутбук с Fine Tuning модели на различных датасетах и замерами метрик
* В папках 'Llama_*' - сохраненные результаты метрик для отрисовки графиков
* QualityTesting/Accuracy_compute.ipynb - файл с написанным жадным inferens, а также замерами Accuracy верных ответов на openai/GSM

